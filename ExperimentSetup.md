## Dataset

* benchmarks: SPIDER [10] and BIRD [20] (conducted in SQLite [21])

## Model and Prompt Selection
* Models
	* the closed-source LLM CHATGPT [22] (GPT-3.5-Turbo) 
	* SQLCODER [23] (llama-3-sqlcoder-8b)

* Prompt Selection
	* DAIL-SQL [24]

which, as of January 2025, has achieved the best performance

on the SPIDER and BIRD benchmarks.？？？ All configurations

follow the DAIL-SQL setup, including the nuclear sampling

strategy and a temperature setting of xxx.

1) Taxonomy Annotation: In order to analyze the hal

lucination types in the LLM-generated SQL, we manually

perform open coding [25] on the generated code to obtain

the hallucination taxonomy. This analysis is driven by our

objective to design a robust framework for validating the

correctness of LLM-Generated SQL. Our annotation process

comprises three main stages:

(1) Initial Open Coding. In the first stage, we randomly

select 10% of the NL2SQL tasks from the SPIDER and BIRD

datasets for a preliminary analysis. For each selected task,

multiple SQL snippets are generated by the evaluated models

and executed in the corresponding SQLite environment to

verify their correctness. Two researchers then compare the

LLM-generated SQL against the ground-truth SQL, identify

ing discrepancies and noting any hallucination phenomena.

(2) Preliminary Taxonomy Construction. Based on the

initial analysis, we document the various types of SQL errors

and their corresponding root causes. Given that a single SQL

snippet may exhibit multiple hallucinations, the researchers

collaboratively classify similar errors to build a preliminary

taxonomy that captures both the manifestations and underlying

causes of the observed errors.

(3) Full Taxonomy Construction. With the preliminary

taxonomy established, the remaining SQL snippets are inde

pendently annotated by three experts with extensive experi

ence in SQL and database management. Any discrepancies

in the annotations are resolved through discussion, and new

error types are incorporated as needed. This comprehensive

taxonomy not only categorizes the diverse error forms in

the LLM-generated SQL but also provides critical insights

into their root causes, thereby laying the groundwork for our

framework to detect the correctness of real-world queries. This

manual analysis process, carried out by five domain experts,

required considerable domain-specific knowledge of databases

and SQL, and consumed approximately 600 hours in total.